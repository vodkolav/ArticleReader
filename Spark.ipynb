{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3817f393-5aa0-4395-b4d7-32aebfce613f",
   "metadata": {},
   "source": [
    "# Big Data Project: Transforming Scientific Articles into Videos with Speech using Apache Spark and Kafka\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c3735a-37f8-4aa4-8853-5fe628b0bb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make modules from py files auto-reload when changed\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78f997a8-4980-480a-892f-d0078db1a876",
   "metadata": {},
   "source": [
    "#TODO:   \n",
    "create folders for data, checkpoints, output, dbg\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "13692290-f4df-474f-b861-095623f1f808",
   "metadata": {},
   "source": [
    "# TTS \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "598aff3b",
   "metadata": {},
   "source": [
    "## Basic run just for test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3012adb6-6d2c-49a4-8e04-eda485d271dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "from ArticleReader.Chunker import Chunker\n",
    "from ArticleReader.LatexToSpeech import LatexParser\n",
    "from ArticleReader.Narrator import Narrator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2b1a90-97c5-42c5-be11-08cc4fdb709f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"data/arXiv-2106.04624v1/main.tex\"\n",
    "output_file = \"output/\" + datetime.now().strftime(r\"%y.%m.%d-%H\")\n",
    "\n",
    "parser = LatexParser()\n",
    "content = parser.read_latex(input_file)\n",
    "processed = parser.custom_latex_to_text(content)\n",
    "parser.save_text(processed, \"dbg/spec_my.txt\")\n",
    "\n",
    "tables = parser.get_tables()\n",
    "parser.save_text(tables, \"dbg/tables.tex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8688131-5656-4c4a-9525-75634809aaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker = Chunker(max_len=200)\n",
    "chunker.split_text_into_chunks(processed)\n",
    "chunks = chunker.get_test_batch(10, 0)\n",
    "# chunks = chunker.chunks\n",
    "chunker.save_chunks_as_text(output_file + \".md\", chunks)\n",
    "print(\"text chunks:\", [len(ch) for ch in chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c389ecf-b341-411b-8536-4fb88408cad4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7164fb19",
   "metadata": {},
   "source": [
    "# Spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe51445",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import findspark\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd \n",
    "import torch\n",
    "\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType, udf, col, lit, desc, floor\n",
    "from pyspark.sql.functions import collect_list, flatten\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import pandas as psp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2beffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc8dca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccf9d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONPATH'] = '/usr/local/spark/spark/python/lib/py4j-0.10.9.7-src.zip:/usr/local/spark/spark/python/:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a82ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Spark environment variables\n",
    "print(os.environ['SPARK_HOME'] )\n",
    "print(os.environ['PYSPARK_PYTHON'])\n",
    "print(os.environ['PYSPARK_DRIVER_PYTHON'] )\n",
    "print(os.environ['PYTHONPATH'] )\n",
    "#print(os.environ['JAVA_HOME'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea57c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b25cbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get number of CPU cores\n",
    "# workers = 1\n",
    "# cpus_limit = os.cpu_count() -1 \n",
    "# mem_limit = \"1g\" # prod: \"16g\"\n",
    "\n",
    "# simulating a cluster with 2 workers\n",
    "workers = 2\n",
    "cpus_limit =  int(os.cpu_count()/ workers) -1 \n",
    "mem_limit = \"2g\" # prod: \"16g\"/ workers\n",
    "\n",
    "\n",
    "\n",
    "# Configure PyTorch for CPU parallelism\n",
    "torch.set_num_threads(cpus_limit)\n",
    "\n",
    "# Create Spark session with configurations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TTS CPU Inference\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.executor.cores\", cpus_limit) \\\n",
    "    .config(\"spark.executor.instances\", workers) \\\n",
    "    .config(\"spark.executor.memory\", mem_limit) \\\n",
    "    .config(\"spark.task.cpus\", cpus_limit) \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"false\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "    # Additional configs that might be useful in future:\n",
    "    # .config(\"spark.executor.resource.gpu.amount\", \"1\") \\\n",
    "    # .config(\"spark.executor.memoryOverhead\", \"<memory>\"\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Using {cpus_limit} CPU cores for inference.\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "42e24b3c",
   "metadata": {},
   "source": [
    "1. Memory Allocation and Executor Configuration\n",
    "\n",
    "    Spark executors are allocated a fixed amount of memory when the application starts. This memory is split between:\n",
    "        JVM heap memory: For Spark tasks and operations.\n",
    "        Off-heap memory: For operations like shuffle or caching.\n",
    "        GPU memory (if applicable): Used by PyTorch or other frameworks.\n",
    "    Memory allocation is configured using:\n",
    "\n",
    "--executor-memory <memory>\n",
    "--driver-memory <memory>\n",
    "--conf spark.memory.fraction=<fraction>\n",
    "\n",
    "2. Dynamic Resource Allocation\n",
    "\n",
    "    Spark can adjust resource allocation dynamically if dynamic resource allocation is enabled.\n",
    "    Configuration:\n",
    "\n",
    "```\n",
    "--conf spark.dynamicAllocation.enabled=true\n",
    "--conf spark.dynamicAllocation.minExecutors=<min>\n",
    "--conf spark.dynamicAllocation.maxExecutors=<max>\n",
    "```\n",
    "3. Task Partitioning and Locality\n",
    "\n",
    "    Spark divides the dataset into partitions, which determine the unit of work assigned to each executor.\n",
    "    Smaller partitions reduce the likelihood of memory overload but increase overhead. Aim for ~128MB partition sizes for large datasets.\n",
    "    Control partition size with:\n",
    "```\n",
    "--conf spark.sql.files.maxPartitionBytes=<size>\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e13a670",
   "metadata": {},
   "source": [
    "## Playground for batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af3dc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3b2f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "waveforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce425a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_lens = narrator.add_pauses(sentences.sentence, mel_lengths, pause_dur=40)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9146fc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733ce8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dd560e",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = torch.tensor_split(waveforms.squeeze(1), len(waveforms), dim=0)\n",
    "\n",
    "#arr = [a[:, :l] for a, l in zip(arr, mel_lengths * self.hop_len)]\n",
    "\n",
    "# TODO: cut padding \n",
    "arr = [a[:, :l].squeeze(0).numpy() for a, l in zip(arr, mel_lens * narrator.hop_len)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea6e171",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1c210f",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(arr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3977cdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr[1].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f092c8b",
   "metadata": {},
   "source": [
    "## Pandas UDF approach \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee0626b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@pandas_udf(StructType([StructField(\"waveform\", ArrayType(FloatType())),StructField(\"mel_lengths\", IntegerType())]))\n",
    "def predict_batch_udf(sentences: pd.Series) -> pd.DataFrame:\n",
    "  # TODO: calculate and store \"seq_len\"\n",
    "  # TODO: non-default model initialization\n",
    "  narrator = Narrator()    \n",
    "  waveforms, mel_lengths = narrator.infer(sentences)\n",
    "\n",
    "  arr = torch.tensor_split(waveforms.squeeze(1), len(waveforms), dim=0)\n",
    "\n",
    "  # Add more pause where needed (very naive currenty)\n",
    "  mel_lengths = narrator.add_pauses(sentences, mel_lengths, pause_dur=40)   \n",
    "  # Cut silence padding while applying pauses from above \n",
    "  arr = [a[:, :l].squeeze(0).numpy() for a, l in zip(arr, mel_lengths * narrator.hop_len)]  \n",
    "  \n",
    "  output = pd.DataFrame({\"waveform\": arr, \"mel_lengths\": mel_lengths})\n",
    " \n",
    "  return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7e1e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# psp.from_pandas(\n",
    "chunks = spark.createDataFrame(chunker.get_test_batch(15,295))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4ee18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the UDF\n",
    "text_volume_window = (Window.orderBy(desc('text_len'))\n",
    "             .rowsBetween(Window.unboundedPreceding, 0))\n",
    "# TODO: maybe can use partitionng here for separating whole text into chapters? \n",
    "\n",
    "text_volume_max  = 600 \n",
    "\n",
    "step1 = chunks.withColumn('cum_text_volume', F.sum('text_len').over(text_volume_window)) \\\n",
    ".withColumn('part', floor(col('cum_text_volume')/lit(text_volume_max)) )\n",
    "step1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaa5792",
   "metadata": {},
   "outputs": [],
   "source": [
    "nparts =  step1.select((lit(1) + F.max(\"part\")).alias(\"npart\")).first()\n",
    "nparts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bec309",
   "metadata": {},
   "outputs": [],
   "source": [
    "nparts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c8eb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "step1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959915d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "step1 = step1.repartitionByRange(nparts[0], \"part\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371b4407",
   "metadata": {},
   "outputs": [],
   "source": [
    "step1.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883b69f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "step2 = step1.withColumn(\"prediction\", predict_batch_udf(col(\"sentence\"))).cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb873b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "step3 = step2.select(\"index\", \"sentence\", \"text_len\", \"prediction.*\")\n",
    "\n",
    "step4 = step3.sort(\"index\")\n",
    "\n",
    "print(\"recombine batch\")\n",
    "wf = step4.agg(flatten(collect_list(col(\"waveform\")))).alias(\"speech\")\n",
    "\n",
    "#torch.cat(tuple(batch_converted.waveform), dim=1)\n",
    "\n",
    "final = step4.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb21ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "step4.select(F.size(\"waveform\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0ac368",
   "metadata": {},
   "outputs": [],
   "source": [
    "final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fe2b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "wfc = wf.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e5b0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wfc.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c86add9",
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform = wfc.collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca52c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "narrator = Narrator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3f9a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "tens = torch.Tensor(waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2111e6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b02696f",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_file = \"output/spark_test\"\n",
    "\n",
    "\n",
    "print(\"saving sound\")\n",
    "narrator.save_audio(case_file + \".wav\",tens )\n",
    "print(\"done saving sound\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007a660c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_batch_size(memory_per_sentence, total_memory, safety_factor=0.8):\n",
    "    # safety_factor reserves a buffer (e.g., 80% of total memory)\n",
    "    return int((total_memory * safety_factor) / memory_per_sentence)\n",
    "\n",
    "batch_size = calculate_batch_size(memory_per_sentence=500_000_000, total_memory=16_000_000_000)\n",
    "batch_size\n",
    "# Result: batch_size = 25"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6225add0",
   "metadata": {},
   "source": [
    "## Regular UDF approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777344c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chunks = psp.from_pandas(chunker.get_test_batch(3,53))\n",
    "#chunks = spark.from_pandas(chunker.chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbac1e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9426206c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bb08a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Estimate batch size\n",
    "batch_size = calculate_batch_size(memory_per_sentence=500_000_000, total_memory=16_000_000_000)\n",
    "batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c13003",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054604c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Repartition the DataFrame to match the batch size\n",
    "num_partitions = max(1, chunks.sentence.count() // batch_size)\n",
    "num_partitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261292df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from pyspark import SparkContext\n",
    "# import torch\n",
    "\n",
    "# sc = SparkContext.getOrCreate()\n",
    "\n",
    "# # Load your PyTorch model\n",
    "# model = torch.load('path_to_model.pth', map_location='cpu')\n",
    "# model.eval()\n",
    "\n",
    "# Broadcast the model\n",
    "#broadcast_model = sc.broadcast(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac16413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the structure and naming for newly created columns\n",
    "schema = StructType([StructField(\"waveforms\",\n",
    "                                 IntegerType(), False),\n",
    "                     StructField(\"mel_lengths\",\n",
    "                                 IntegerType(), False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ccd7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf('string', PandasUDFType.SCALAR)\n",
    "def tts_udf(batch):\n",
    "    narrator = Narrator()    \n",
    "    waveforms, mel_lengths = narrator.infer(batch)    \n",
    "    return waveforms, mel_lengths\n",
    "\n",
    "\n",
    "    # ensure sentences are sorted by seq_len\n",
    "    batch_df.loc[:,\"seq_len\"] = batch_df.sentence.map(narrator.seq_len)\n",
    "    batch_df.sort_values(\"seq_len\", ascending=False, inplace=True)\n",
    "\n",
    "    # # defining pauses between paragraphs                \n",
    "    # mel_lengths = narrator.add_pauses(batch_df.sentence, mel_lengths, pause_dur=40)        \n",
    "\n",
    "    # # turning tensor into regular array\n",
    "    # arr = torch.tensor_split(waveforms.squeeze(1), len(waveforms), dim=0)\n",
    "\n",
    "    # # cut padding\n",
    "    # arr = [a[:, :l] for a, l in zip(arr, mel_lengths * narrator.hop_len)]\n",
    "    \n",
    "    # mel_lengths = mel_lengths.detach().numpy()\n",
    "    # batch_df[\"waveform\"] = arr\n",
    "    # batch_df[\"mel_lengths\"] = mel_lengths\n",
    "    # batch_df[\"durations_sec\"] = mel_lengths / 22050.0\n",
    "    # return batch_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970fe7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the UDF\n",
    "step1 = chunks.to_spark().sort(\"text_len\", ascending=False ) \n",
    "step1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875630eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(step1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0b87bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "step2 = step1.withColumn(\"Result\", tts_udf(\"sentence\")) \n",
    "step2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316b6344",
   "metadata": {},
   "outputs": [],
   "source": [
    "step3 = step2.select(\"index\", \"sentence\", \"text_len\", \"Result.*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bc19cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf('string', PandasUDFType.SCALAR)\n",
    "def tts_udf(batch):\n",
    "    model = broadcast_model.value.to('cuda')\n",
    "    memory_per_sentence = 500_000_000  # Estimate\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory * 0.8\n",
    "    batch_size = int(total_memory / memory_per_sentence)\n",
    "\n",
    "    results = []\n",
    "    for i in range(0, len(batch), batch_size):\n",
    "        batch = batch[i:i + batch_size]\n",
    "        with torch.no_grad():\n",
    "            inputs = [torch.tensor(sentence).to('cuda') for sentence in batch]\n",
    "            outputs = model(inputs)\n",
    "            results.extend(outputs.cpu().numpy())\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9d3237",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to process sentences in a batch\n",
    "def process_batch(partition):\n",
    "    model = broadcast_model.value.to('cuda')\n",
    "    results = []\n",
    "    for sentence in partition:\n",
    "        with torch.no_grad():\n",
    "            output = model(sentence)\n",
    "            results.append(output.cpu().numpy())\n",
    "    return iter(results)\n",
    "\n",
    "# Apply the function to each partition\n",
    "processed_rdd = df.rdd.mapPartitions(process_batch)\n",
    "result_df = processed_rdd.toDF([\"processed_output\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f0016f49",
   "metadata": {},
   "source": [
    "Set the executor's memory and GPU resources:\n",
    "\n",
    "```\n",
    "--conf spark.executor.memory=16G\n",
    "--conf spark.executor.resource.gpu.amount=1\n",
    "```\n",
    "\n",
    "Adjust the number of partitions:\n",
    "```\n",
    "--conf spark.sql.shuffle.partitions=<num_partitions>\n",
    "```\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44d9e29d-342a-416e-963b-d20ef8983cfc",
   "metadata": {},
   "source": [
    "# End"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "550f119b",
   "metadata": {},
   "source": [
    "# Trash "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd54de80",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_df = chunker.get_test_batch(3,53)\n",
    "batch_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76584cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_df.loc[:,\"seq\"] = batch_df.sentence.map(narrator.tts.text_to_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e264f203",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ed6ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_df.seq.map(lambda s: len(s[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe68ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = batch_df.sort_values(\"text_len\" ,ascending = False).reset_index()\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6644ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "narrator = Narrator()    \n",
    "waveforms, mel_lengths = narrator.infer(sentences.sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57ae8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "waveforms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76674434",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = torch.tensor_split(waveforms.squeeze(1), len(waveforms), dim=0)\n",
    "arr = [a[0].squeeze(0).numpy() for a in arr]\n",
    "# cut padding\n",
    "#arr = [a[:, :l] for a, l in zip(arr, mel_lengths * narrator.hop_len)]\n",
    "\n",
    "#mel_lengths = mel_lengths.detach().numpy()\n",
    "# batch_df[\"waveform\"] = arr\n",
    "# batch_df[\"mel_lengths\"] = mel_lengths\n",
    "# batch_df[\"durations_sec\"] = mel_lengths / 22050.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864b667a",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e3e4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "output = pd.DataFrame({\"waveform\": arr, \"mel_lengths\": mel_lengths})\n",
    "# sentences[\"waveforms\"] = waveforms\n",
    "# sentences[\"mel_lengths\"] = mel_lengths    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079947d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2362a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217a7cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.loc[0,\"waveform\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyspark_TTS]",
   "language": "python",
   "name": "conda-env-pyspark_TTS-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
