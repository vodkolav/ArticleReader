{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3817f393-5aa0-4395-b4d7-32aebfce613f",
   "metadata": {},
   "source": [
    "# Big Data Project: Transforming Scientific Articles into Videos with Speech using Apache Spark and Kafka\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62c3735a-37f8-4aa4-8853-5fe628b0bb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make modules from py files auto-reload when changed\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78f997a8-4980-480a-892f-d0078db1a876",
   "metadata": {},
   "source": [
    "#TODO:   \n",
    "create folders for data, checkpoints, output, dbg\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "13692290-f4df-474f-b861-095623f1f808",
   "metadata": {},
   "source": [
    "# TTS \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "598aff3b",
   "metadata": {},
   "source": [
    "## Basic run just for test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3012adb6-6d2c-49a4-8e04-eda485d271dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linuxu/anaconda3/envs/pyspark_TTS/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:speechbrain.utils.quirks:Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]\n",
      "INFO:speechbrain.utils.quirks:Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n",
      "/home/linuxu/anaconda3/envs/pyspark_TTS/lib/python3.9/site-packages/IPython/extensions/autoreload.py:211: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  if not hasattr(module, \"__file__\") or module.__file__ is None:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "from ArticleReader.Chunker import Chunker\n",
    "from ArticleReader.LatexToSpeech import LatexParser\n",
    "from ArticleReader.Narrator import Narrator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f2b1a90-97c5-42c5-be11-08cc4fdb709f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"data/arXiv-2106.04624v1/main.tex\"\n",
    "output_file = \"output/\" + datetime.now().strftime(r\"%y.%m.%d-%H\")\n",
    "\n",
    "parser = LatexParser()\n",
    "content = parser.read_latex(input_file)\n",
    "processed = parser.custom_latex_to_text(content)\n",
    "parser.save_text(processed, \"dbg/spec_my.txt\")\n",
    "\n",
    "tables = parser.get_tables()\n",
    "parser.save_text(tables, \"dbg/tables.tex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8688131-5656-4c4a-9525-75634809aaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker = Chunker(max_len=200)\n",
    "chunker.split_text_into_chunks(processed)\n",
    "n_chunks = chunker.get_test_batch(10, 0)\n",
    "# chunks = chunker.chunks\n",
    "chunker.save_chunks_as_text(output_file + \".md\", n_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c389ecf-b341-411b-8536-4fb88408cad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>sentence</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>si plus plus</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>\\n\\n</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n ...</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\\n\\n</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>SpeechBrain: A General-Purpose Speech Toolkit.</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>\\n\\n</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>SpeechBrain is an open-source and all-in-one s...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>It is designed to facilitate the research and ...</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>\\nThis paper describes the core architecture d...</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>SpeechBrain achieves competitive or state-of-t...</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                           sentence  text_len\n",
       "0      0                                       si plus plus        12\n",
       "1      1                                               \\n\\n         4\n",
       "2      2   \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n ...        40\n",
       "3      3                                               \\n\\n         4\n",
       "4      4     SpeechBrain: A General-Purpose Speech Toolkit.        46\n",
       "5      5                                               \\n\\n         4\n",
       "6      6  SpeechBrain is an open-source and all-in-one s...        61\n",
       "7      7  It is designed to facilitate the research and ...       162\n",
       "8      8  \\nThis paper describes the core architecture d...       189\n",
       "9      9  SpeechBrain achieves competitive or state-of-t...       103"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7164fb19",
   "metadata": {},
   "source": [
    "# Spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbe51445",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linuxu/anaconda3/envs/pyspark_TTS/lib/python3.9/site-packages/pyspark/pandas/__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#import findspark\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd \n",
    "import torch\n",
    "\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType, udf, col, lit, desc, floor\n",
    "from pyspark.sql.functions import collect_list, flatten\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import pandas as psp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a2beffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/linuxu/anaconda3/envs/pyspark_TTS/bin/python'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fc8dca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/linuxu/Michael/BigData/ArticleReader'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ccf9d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONPATH'] = '/usr/local/spark/spark/python/lib/py4j-0.10.9.7-src.zip:/usr/local/spark/spark/python/:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89a82ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/spark\n",
      "/home/linuxu/anaconda3/envs/pyspark_TTS/bin/python\n",
      "ipython\n",
      "/usr/local/spark/spark/python/lib/py4j-0.10.9.7-src.zip:/usr/local/spark/spark/python/:\n"
     ]
    }
   ],
   "source": [
    "# Set Spark environment variables\n",
    "print(os.environ['SPARK_HOME'] )\n",
    "print(os.environ['PYSPARK_PYTHON'])\n",
    "print(os.environ['PYSPARK_DRIVER_PYTHON'] )\n",
    "print(os.environ['PYTHONPATH'] )\n",
    "#print(os.environ['JAVA_HOME'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ea57c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "environ{'CHROME_DESKTOP': 'code-url-handler.desktop',\n",
       "        'DBUS_SESSION_BUS_ADDRESS': 'unix:path=/run/user/1000/bus,guid=99a19c09a7fc26df45954dbf67cae258',\n",
       "        'DBUS_STARTER_ADDRESS': 'unix:path=/run/user/1000/bus,guid=99a19c09a7fc26df45954dbf67cae258',\n",
       "        'DBUS_STARTER_BUS_TYPE': 'session',\n",
       "        'DESKTOP_SESSION': 'ubuntu',\n",
       "        'DISPLAY': ':100',\n",
       "        'GDK_BACKEND': 'x11',\n",
       "        'GDMSESSION': 'ubuntu',\n",
       "        'GIO_LAUNCHED_DESKTOP_FILE': '/usr/share/applications/code.desktop',\n",
       "        'GIO_LAUNCHED_DESKTOP_FILE_PID': '14923',\n",
       "        'GNOME_DESKTOP_SESSION_ID': 'this-is-deprecated',\n",
       "        'GNOME_SHELL_SESSION_MODE': 'ubuntu',\n",
       "        'GPG_AGENT_INFO': '/run/user/1000/gnupg/S.gpg-agent:0:1',\n",
       "        'GTK_IM_MODULE': 'ibus',\n",
       "        'GTK_MODULES': 'gail:atk-bridge',\n",
       "        'HOME': '/home/linuxu',\n",
       "        'INVOCATION_ID': 'a336489572c445eaad1bd3de0cfdb8fd',\n",
       "        'JOURNAL_STREAM': '8:145702',\n",
       "        'LANG': 'en_IL',\n",
       "        'LANGUAGE': 'en_IL:en',\n",
       "        'LOGNAME': 'linuxu',\n",
       "        'MANAGERPID': '11262',\n",
       "        'ORIGINAL_XDG_CURRENT_DESKTOP': 'ubuntu:GNOME',\n",
       "        'PATH': '/home/linuxu/anaconda3/envs/pyspark_TTS/bin:/usr/bin:/usr/bin:/usr/bin:/home/linuxu/anaconda3/envs/pyspark_TTS/bin:/home/linuxu/anaconda3/condabin:/usr/local/cuda-11.7/bin:/usr/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/spark/spark/bin:/usr/local/kafka/kafka/bin:/usr/local/spark/spark/bin:/usr/local/kafka/kafka/bin:/usr/local/spark/spark/bin:/usr/local/kafka/kafka/bin:/usr/local/spark/spark/bin:/usr/local/kafka/kafka/bin',\n",
       "        'PWD': '/home/linuxu',\n",
       "        'QT_ACCESSIBILITY': '1',\n",
       "        'QT_IM_MODULE': 'ibus',\n",
       "        'SESSION_MANAGER': 'local/Ubuntu-GPU-63:@/tmp/.ICE-unix/11634,unix/Ubuntu-GPU-63:/tmp/.ICE-unix/11634',\n",
       "        'SHELL': '/bin/bash',\n",
       "        'SHLVL': '1',\n",
       "        'SSH_AGENT_LAUNCHER': 'gnome-keyring',\n",
       "        'SSH_AUTH_SOCK': '/run/user/1000/keyring/ssh',\n",
       "        'SYSTEMD_EXEC_PID': '11634',\n",
       "        'USER': 'linuxu',\n",
       "        'USERNAME': 'linuxu',\n",
       "        'VSCODE_AMD_ENTRYPOINT': 'vs/workbench/api/node/extensionHostProcess',\n",
       "        'VSCODE_CODE_CACHE_PATH': '/home/linuxu/.config/Code/CachedData/b3e4e68a0bc097f0ae7907b217c1119af9e03435',\n",
       "        'VSCODE_CRASH_REPORTER_PROCESS_TYPE': 'extensionHost',\n",
       "        'VSCODE_CWD': '/home/linuxu',\n",
       "        'VSCODE_HANDLES_UNCAUGHT_ERRORS': 'true',\n",
       "        'VSCODE_IPC_HOOK': '/run/user/1000/vscode-2148651c-1.78-main.sock',\n",
       "        'VSCODE_NLS_CONFIG': '{\"locale\":\"en-gb\",\"osLocale\":\"en-il\",\"availableLanguages\":{},\"_languagePackSupport\":true}',\n",
       "        'VSCODE_PID': '14923',\n",
       "        'XAUTHORITY': '/run/gdm3/auth-for-linuxu-06PY22/database',\n",
       "        'XDG_CONFIG_DIRS': '/etc/xdg/xdg-ubuntu:/etc/xdg',\n",
       "        'XDG_CURRENT_DESKTOP': 'Unity',\n",
       "        'XDG_DATA_DIRS': '/usr/share/ubuntu:/usr/share/gnome:/usr/local/share:/usr/share:/var/lib/snapd/desktop',\n",
       "        'XDG_MENU_PREFIX': 'gnome-',\n",
       "        'XDG_RUNTIME_DIR': '/run/user/1000',\n",
       "        'XDG_SESSION_CLASS': 'user',\n",
       "        'XDG_SESSION_DESKTOP': 'ubuntu',\n",
       "        'XDG_SESSION_TYPE': 'x11',\n",
       "        'XMODIFIERS': '@im=ibus',\n",
       "        '_': '/home/linuxu/anaconda3/envs/pyspark_TTS/bin/python',\n",
       "        'ELECTRON_RUN_AS_NODE': '1',\n",
       "        'VSCODE_L10N_BUNDLE_LOCATION': '',\n",
       "        'APPLICATION_INSIGHTS_NO_DIAGNOSTIC_CHANNEL': 'true',\n",
       "        'PYTHONUNBUFFERED': '1',\n",
       "        'CONDA_EXE': '/home/linuxu/anaconda3/bin/conda',\n",
       "        'PYSPARK_DRIVER_PYTHON': 'ipython',\n",
       "        'XML_CATALOG_FILES': 'file:///home/linuxu/anaconda3/envs/pyspark_TTS/etc/xml/catalog file:///etc/xml/catalog',\n",
       "        'GSETTINGS_SCHEMA_DIR': '/home/linuxu/anaconda3/envs/pyspark_TTS/share/glib-2.0/schemas',\n",
       "        'CONDA_ROOT': '/home/linuxu/anaconda3',\n",
       "        'CONDA_PREFIX': '/home/linuxu/anaconda3/envs/pyspark_TTS',\n",
       "        'GSETTINGS_SCHEMA_DIR_CONDA_BACKUP': '',\n",
       "        '__CONDA_SHLVL_1_PATH': '/home/linuxu/anaconda3/bin:/home/linuxu/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin',\n",
       "        'KAFKA_HOME': '/usr/local/kafka/kafka',\n",
       "        'CONDA_PROMPT_MODIFIER': '(pyspark_TTS) ',\n",
       "        'PYSPARK_PYTHON': '/home/linuxu/anaconda3/envs/pyspark_TTS/bin/python',\n",
       "        'PYTHONIOENCODING': 'utf-8',\n",
       "        'CONDA_SHLVL': '2',\n",
       "        'SPARK_HOME': '/usr/local/spark/spark',\n",
       "        'CONDA_PYTHON_EXE': '/home/linuxu/anaconda3/bin/python',\n",
       "        'CONDA_DEFAULT_ENV': 'pyspark_TTS',\n",
       "        'CONDA_ALLOW_SOFTLINKS': 'false',\n",
       "        'CONDA_PREFIX_1': '/home/linuxu/anaconda3',\n",
       "        'PYDEVD_IPYTHON_COMPATIBLE_DEBUGGING': '1',\n",
       "        'PYDEVD_USE_FRAME_EVAL': 'NO',\n",
       "        'TERM': 'xterm-color',\n",
       "        'CLICOLOR': '1',\n",
       "        'FORCE_COLOR': '1',\n",
       "        'CLICOLOR_FORCE': '1',\n",
       "        'PAGER': 'cat',\n",
       "        'GIT_PAGER': 'cat',\n",
       "        'MPLBACKEND': 'module://matplotlib_inline.backend_inline',\n",
       "        'SB_LOG_LEVEL': '20',\n",
       "        'PYARROW_IGNORE_TIMEZONE': '1',\n",
       "        'PYTHONPATH': '/usr/local/spark/spark/python/lib/py4j-0.10.9.7-src.zip:/usr/local/spark/spark/python/:'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b25cbd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/13 18:17:54 WARN Utils: Your hostname, Ubuntu-GPU-63 resolves to a loopback address: 127.0.1.1; using 192.168.90.110 instead (on interface ens160)\n",
      "25/03/13 18:17:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/13 18:17:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.4\n",
      "Using 7 CPU cores for inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.90.110:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>TTS CPU Inference</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=TTS CPU Inference>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Get number of CPU cores\n",
    "# workers = 1\n",
    "# cpus_limit = os.cpu_count() -1 \n",
    "# mem_limit = \"1g\" # prod: \"16g\"\n",
    "\n",
    "# simulating a cluster with 2 workers\n",
    "workers = 2\n",
    "cpus_limit =  int(os.cpu_count()/ workers) -1 \n",
    "mem_limit = \"2g\" # prod: \"16g\"/ workers\n",
    "\n",
    "\n",
    "\n",
    "# Configure PyTorch for CPU parallelism\n",
    "torch.set_num_threads(cpus_limit)\n",
    "\n",
    "# Create Spark session with configurations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TTS CPU Inference\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.executor.cores\", cpus_limit) \\\n",
    "    .config(\"spark.executor.instances\", workers) \\\n",
    "    .config(\"spark.executor.memory\", mem_limit) \\\n",
    "    .config(\"spark.task.cpus\", cpus_limit) \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"false\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "    # Additional configs that might be useful in future:\n",
    "    # .config(\"spark.executor.resource.gpu.amount\", \"1\") \\\n",
    "    # .config(\"spark.executor.memoryOverhead\", \"<memory>\"\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Using {cpus_limit} CPU cores for inference.\")\n",
    "\n",
    "spark.sparkContext"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "42e24b3c",
   "metadata": {},
   "source": [
    "1. Memory Allocation and Executor Configuration\n",
    "\n",
    "    Spark executors are allocated a fixed amount of memory when the application starts. This memory is split between:\n",
    "        JVM heap memory: For Spark tasks and operations.\n",
    "        Off-heap memory: For operations like shuffle or caching.\n",
    "        GPU memory (if applicable): Used by PyTorch or other frameworks.\n",
    "    Memory allocation is configured using:\n",
    "\n",
    "--executor-memory <memory>\n",
    "--driver-memory <memory>\n",
    "--conf spark.memory.fraction=<fraction>\n",
    "\n",
    "2. Dynamic Resource Allocation\n",
    "\n",
    "    Spark can adjust resource allocation dynamically if dynamic resource allocation is enabled.\n",
    "    Configuration:\n",
    "\n",
    "```\n",
    "--conf spark.dynamicAllocation.enabled=true\n",
    "--conf spark.dynamicAllocation.minExecutors=<min>\n",
    "--conf spark.dynamicAllocation.maxExecutors=<max>\n",
    "```\n",
    "3. Task Partitioning and Locality\n",
    "\n",
    "    Spark divides the dataset into partitions, which determine the unit of work assigned to each executor.\n",
    "    Smaller partitions reduce the likelihood of memory overload but increase overhead. Aim for ~128MB partition sizes for large datasets.\n",
    "    Control partition size with:\n",
    "```\n",
    "--conf spark.sql.files.maxPartitionBytes=<size>\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e13a670",
   "metadata": {},
   "source": [
    "## Playground for batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af3dc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3b2f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "waveforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce425a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_lens = narrator.add_pauses(sentences.sentence, mel_lengths, pause_dur=40)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9146fc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733ce8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dd560e",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = torch.tensor_split(waveforms.squeeze(1), len(waveforms), dim=0)\n",
    "\n",
    "#arr = [a[:, :l] for a, l in zip(arr, mel_lengths * self.hop_len)]\n",
    "\n",
    "# TODO: cut padding \n",
    "arr = [a[:, :l].squeeze(0).numpy() for a, l in zip(arr, mel_lens * narrator.hop_len)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea6e171",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1c210f",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(arr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3977cdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr[1].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f092c8b",
   "metadata": {},
   "source": [
    "## Pandas UDF approach \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "6ee0626b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@pandas_udf(StructType([StructField(\"waveform\", ArrayType(FloatType())),StructField(\"mel_lengths\", IntegerType())]))\n",
    "def predict_batch_udf(sentences: pd.Series) -> pd.DataFrame:\n",
    "  # TODO: calculate and store \"seq_len\"\n",
    "  # TODO: non-default model initialization\n",
    "  narrator = Narrator()    \n",
    "  waveforms, mel_lengths = narrator.infer(sentences)\n",
    "\n",
    "  arr = torch.tensor_split(waveforms.squeeze(1), len(waveforms), dim=0)\n",
    "\n",
    "  # Add more pause where needed (very naive currenty)\n",
    "  mel_lengths = narrator.add_pauses(sentences, mel_lengths, pause_dur=40)   \n",
    "  # Cut silence padding while applying pauses from above \n",
    "  arr = [a[:, :l].squeeze(0).numpy() for a, l in zip(arr, mel_lengths * narrator.hop_len)]  \n",
    "  \n",
    "  output = pd.DataFrame({\"waveform\": arr, \"mel_lengths\": mel_lengths})\n",
    " \n",
    "  return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f22705b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.90.110:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>TTS CPU Inference</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=TTS CPU Inference>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "34d4d80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_file = \"data/arXiv-2106.04624v1/main.tex\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "71aeb3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(file_content):\n",
    "    parser = LatexParser()\n",
    "    processed_text = parser.custom_latex_to_text(file_content)   \n",
    "\n",
    "    return processed_text, parser.tables, parser.figures\n",
    "    # return { \"text\": processed_text, \n",
    "    #         \"tables\": parser.tables, \n",
    "    #         \"figures\": parser.figures } \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8a7b7187",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sc.wholeTextFiles(input_file)\n",
    "\n",
    "processed_rdd = df.mapValues(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "815313c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Convert RDD to DataFrame\n",
    "from pyspark.sql import Row\n",
    "\n",
    "df_processed = processed_rdd.map(lambda x: Row(filename=x[0], \n",
    "                                               text=x[1][0], \n",
    "                                               tables=x[1][1], \n",
    "                                               figures=x[1][2])).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f6b420d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema matching the Pandas DataFrame structure\n",
    "chunk_schema = StructType([\n",
    "    StructField(\"index\", IntegerType(), False),  \n",
    "    StructField(\"sentence\", StringType(), False),  \n",
    "    StructField(\"text_len\", IntegerType(), False)\n",
    "])\n",
    "\n",
    "# Register as a Pandas UDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b7a0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@pandas_udf(chunk_schema)\n",
    "\n",
    "@udf(ArrayType(StringType()))\n",
    "def udf_split_text(text, chunk_size=500, test =False):\n",
    "    from ArticleReader.Chunker import Chunker\n",
    "    chunker = Chunker(max_len=chunk_size)\n",
    "    chunker.split_text_into_chunks(text)\n",
    "    return chunker.get_chunks()\n",
    "    \n",
    "\n",
    "df_chunks = df_processed.withColumn(\"chunks\", udf_split_text(df_processed[\"text\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "01bcee1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_chunks.selectExpr(\"filename\", \"explode(chunks) as sentence\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "93513975",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id \n",
    "\n",
    "df2 = df_final.withColumn(\"index\", monotonically_increasing_id()) \\\n",
    "    .selectExpr(\"*\",\" length(sentence) as text_len\").offset(295).limit(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "fdb01f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 44:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|count(sentence)|\n",
      "+---------------+\n",
      "|             15|\n",
      "+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df2.selectExpr('count(sentence)').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "8c4ee18d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[filename: string, sentence: string, index: bigint, text_len: int, cum_text_volume: bigint, part: bigint]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the UDF\n",
    "text_volume_window = (Window.orderBy(desc('text_len'))\n",
    "             .rowsBetween(Window.unboundedPreceding, 0))\n",
    "# TODO: maybe can use partitionng here for separating whole text into chapters? \n",
    "\n",
    "text_volume_max  = 600 \n",
    "\n",
    "step1 = df2.withColumn('cum_text_volume', F.sum('text_len').over(text_volume_window)) \\\n",
    ".withColumn('part', floor(col('cum_text_volume')/lit(text_volume_max)) )\n",
    "step1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "3aaa5792",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/13 20:06:39 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/13 20:06:39 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/13 20:06:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/13 20:06:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(npart=3)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nparts =  step1.select((lit(1) + F.max(\"part\")).alias(\"npart\")).first()\n",
    "nparts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "89bec309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nparts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "e2c8eb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/13 20:06:44 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/13 20:06:44 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/13 20:06:44 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Stage 50:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+--------+---------------+----+\n",
      "|            filename|            sentence|index|text_len|cum_text_volume|part|\n",
      "+--------------------+--------------------+-----+--------+---------------+----+\n",
      "|file:/home/linuxu...|In neuroscience, ...|  296|     398|            398|   0|\n",
      "|file:/home/linuxu...|To make inference...|  300|     244|            642|   1|\n",
      "|file:/home/linuxu...|In this case, we ...|  304|     218|            860|   1|\n",
      "|file:/home/linuxu...|In the following,...|  308|     151|           1011|   1|\n",
      "|file:/home/linuxu...|The inference API...|  302|     141|           1152|   1|\n",
      "|file:/home/linuxu...|Subsection: Exper...|  306|      31|           1183|   1|\n",
      "|file:/home/linuxu...|Subsubsection: In...|  298|      25|           1208|   2|\n",
      "|file:/home/linuxu...|                \\n\\n|  295|       4|           1212|   2|\n",
      "|file:/home/linuxu...|                \\n\\n|  297|       4|           1216|   2|\n",
      "|file:/home/linuxu...|                \\n\\n|  299|       4|           1220|   2|\n",
      "|file:/home/linuxu...|                \\n\\n|  301|       4|           1224|   2|\n",
      "|file:/home/linuxu...|                \\n\\n|  303|       4|           1228|   2|\n",
      "|file:/home/linuxu...|                \\n\\n|  305|       4|           1232|   2|\n",
      "|file:/home/linuxu...|                \\n\\n|  307|       4|           1236|   2|\n",
      "|file:/home/linuxu...|                \\n\\n|  309|       4|           1240|   2|\n",
      "+--------------------+--------------------+-----+--------+---------------+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/13 20:06:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/13 20:06:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "step1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "959915d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "step1 = step1.repartitionByRange(nparts[0], \"part\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "371b4407",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/13 20:07:03 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/13 20:07:03 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/13 20:07:04 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/13 20:07:04 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step1.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "883b69f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/13 20:07:44 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/13 20:07:44 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "step2 = step1.withColumn(\"prediction\", predict_batch_udf(col(\"sentence\"))).cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "eb873b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recombine batch\n"
     ]
    }
   ],
   "source": [
    "\n",
    "step3 = step2.select(\"index\", \"sentence\", \"text_len\", \"prediction.*\")\n",
    "\n",
    "step4 = step3.sort(\"index\")\n",
    "\n",
    "print(\"recombine batch\")\n",
    "wf = step4.agg(flatten(collect_list(col(\"waveform\")))).alias(\"speech\")\n",
    "\n",
    "#torch.cat(tuple(batch_converted.waveform), dim=1)\n",
    "\n",
    "final = step4.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "8bb21ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/13 20:08:10 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/13 20:08:10 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "INFO:speechbrain.utils.quirks:Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]\n",
      "INFO:speechbrain.utils.quirks:Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n",
      "INFO:speechbrain.utils.quirks:Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]\n",
      "INFO:speechbrain.utils.quirks:Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n",
      "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/tts-tacotron2-ljspeech' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/tts-tacotron2-ljspeech' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/tts-tacotron2-ljspeech' if not cached\n",
      "25/03/13 20:08:13 WARN BlockManager: Putting block rdd_282_0 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_387938/4270153668.py\", line 5, in predict_batch_udf\n",
      "  File \"/home/linuxu/Michael/BigData/ArticleReader/ArticleReader/Narrator.py\", line 10, in __init__\n",
      "    self.loadModels(tts_model, vocoder_model)\n",
      "  File \"/home/linuxu/Michael/BigData/ArticleReader/ArticleReader/Narrator.py\", line 19, in loadModels\n",
      "    self.tts = Tacotron2.from_hparams(\n",
      "  File \"/home/linuxu/anaconda3/envs/pyspark_TTS/lib/python3.9/site-packages/speechbrain/inference/interfaces.py\", line 472, in from_hparams\n",
      "    hparams_local_path = fetch(\n",
      "  File \"/home/linuxu/anaconda3/envs/pyspark_TTS/lib/python3.9/site-packages/speechbrain/utils/fetching.py\", line 398, in fetch\n",
      "    return link_with_strategy(fetched_file, destination, local_strategy)\n",
      "  File \"/home/linuxu/anaconda3/envs/pyspark_TTS/lib/python3.9/site-packages/speechbrain/utils/fetching.py\", line 162, in link_with_strategy\n",
      "    dst.symlink_to(src)\n",
      "  File \"/home/linuxu/anaconda3/envs/pyspark_TTS/lib/python3.9/pathlib.py\", line 1403, in symlink_to\n",
      "    self._accessor.symlink(target, self, target_is_directory)\n",
      "  File \"/home/linuxu/anaconda3/envs/pyspark_TTS/lib/python3.9/pathlib.py\", line 456, in symlink\n",
      "    return os.symlink(a, b)\n",
      "FileExistsError: [Errno 17] File exists: '/home/linuxu/.cache/huggingface/hub/models--speechbrain--tts-tacotron2-ljspeech/snapshots/d01e530d6d8e1b388c04b882305867addbed4389/hyperparams.yaml' -> '/home/linuxu/Michael/BigData/ArticleReader/checkpoints/tts-tacotron2-ljspeech/hyperparams.yaml'\n",
      ".\n",
      "25/03/13 20:08:13 WARN BlockManager: Block rdd_282_0 could not be removed as it was not found on disk or in memory\n",
      "25/03/13 20:08:13 ERROR Executor: Exception in task 0.0 in stage 65.0 (TID 49)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_387938/4270153668.py\", line 5, in predict_batch_udf\n",
      "  File \"/home/linuxu/Michael/BigData/ArticleReader/ArticleReader/Narrator.py\", line 10, in __init__\n",
      "    self.loadModels(tts_model, vocoder_model)\n",
      "  File \"/home/linuxu/Michael/BigData/ArticleReader/ArticleReader/Narrator.py\", line 19, in loadModels\n",
      "    self.tts = Tacotron2.from_hparams(\n",
      "  File \"/home/linuxu/anaconda3/envs/pyspark_TTS/lib/python3.9/site-packages/speechbrain/inference/interfaces.py\", line 472, in from_hparams\n",
      "    hparams_local_path = fetch(\n",
      "  File \"/home/linuxu/anaconda3/envs/pyspark_TTS/lib/python3.9/site-packages/speechbrain/utils/fetching.py\", line 398, in fetch\n",
      "    return link_with_strategy(fetched_file, destination, local_strategy)\n",
      "  File \"/home/linuxu/anaconda3/envs/pyspark_TTS/lib/python3.9/site-packages/speechbrain/utils/fetching.py\", line 162, in link_with_strategy\n",
      "    dst.symlink_to(src)\n",
      "  File \"/home/linuxu/anaconda3/envs/pyspark_TTS/lib/python3.9/pathlib.py\", line 1403, in symlink_to\n",
      "    self._accessor.symlink(target, self, target_is_directory)\n",
      "  File \"/home/linuxu/anaconda3/envs/pyspark_TTS/lib/python3.9/pathlib.py\", line 456, in symlink\n",
      "    return os.symlink(a, b)\n",
      "FileExistsError: [Errno 17] File exists: '/home/linuxu/.cache/huggingface/hub/models--speechbrain--tts-tacotron2-ljspeech/snapshots/d01e530d6d8e1b388c04b882305867addbed4389/hyperparams.yaml' -> '/home/linuxu/Michael/BigData/ArticleReader/checkpoints/tts-tacotron2-ljspeech/hyperparams.yaml'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:119)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:288)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/03/13 20:08:13 WARN TaskSetManager: Lost task 0.0 in stage 65.0 (TID 49) (192.168.90.110 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_387938/4270153668.py\", line 5, in predict_batch_udf\n",
      "  File \"/home/linuxu/Michael/BigData/ArticleReader/ArticleReader/Narrator.py\", line 10, in __init__\n",
      "    self.loadModels(tts_model, vocoder_model)\n",
      "  File \"/home/linuxu/Michael/BigData/ArticleReader/ArticleReader/Narrator.py\", line 19, in loadModels\n",
      "    self.tts = Tacotron2.from_hparams(\n",
      "  File \"/home/linuxu/anaconda3/envs/pyspark_TTS/lib/python3.9/site-packages/speechbrain/inference/interfaces.py\", line 472, in from_hparams\n",
      "    hparams_local_path = fetch(\n",
      "  File \"/home/linuxu/anaconda3/envs/pyspark_TTS/lib/python3.9/site-packages/speechbrain/utils/fetching.py\", line 398, in fetch\n",
      "    return link_with_strategy(fetched_file, destination, local_strategy)\n",
      "  File \"/home/linuxu/anaconda3/envs/pyspark_TTS/lib/python3.9/site-packages/speechbrain/utils/fetching.py\", line 162, in link_with_strategy\n",
      "    dst.symlink_to(src)\n",
      "  File \"/home/linuxu/anaconda3/envs/pyspark_TTS/lib/python3.9/pathlib.py\", line 1403, in symlink_to\n",
      "    self._accessor.symlink(target, self, target_is_directory)\n",
      "  File \"/home/linuxu/anaconda3/envs/pyspark_TTS/lib/python3.9/pathlib.py\", line 456, in symlink\n",
      "    return os.symlink(a, b)\n",
      "FileExistsError: [Errno 17] File exists: '/home/linuxu/.cache/huggingface/hub/models--speechbrain--tts-tacotron2-ljspeech/snapshots/d01e530d6d8e1b388c04b882305867addbed4389/hyperparams.yaml' -> '/home/linuxu/Michael/BigData/ArticleReader/checkpoints/tts-tacotron2-ljspeech/hyperparams.yaml'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:119)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:288)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "25/03/13 20:08:13 ERROR TaskSetManager: Task 0 in stage 65.0 failed 1 times; aborting job\n",
      "25/03/13 20:08:13 WARN BlockManager: Putting block rdd_282_2 failed due to exception org.apache.spark.TaskKilledException.\n",
      "25/03/13 20:08:13 WARN BlockManager: Block rdd_282_2 could not be removed as it was not found on disk or in memory\n",
      "25/03/13 20:08:13 WARN TaskSetManager: Lost task 2.0 in stage 65.0 (TID 51) (192.168.90.110 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 65.0 failed 1 times, most recent failure: Lost task 0.0 in stage 65.0 (TID 49) (192.168.90.110 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_387938/4270153668.py\", line 5, in predict_batch_udf\n",
      "  File \"/home/linuxu/Michael/BigData/ArticleReader/ArticleReader/Narrator.py\", line 10, in __init__\n",
      "    self.loadModels(tts_model, vocoder_model)\n",
      "  File \"/home/linuxu/Michael/BigData/ArticleReader/ArticleReader/Narrator.py\", line 19, in loadModels\n",
      "    self.tts = Tacotron2.from_hparams(\n",
      "  File \"/home/linuxu/anaconda3/envs/pyspark_TTS/lib/python3.9/site-packages/speechbrain/inference/interfaces.py\", line 472, in from_hparams\n",
      "    hparams_local_path = fetch(\n",
      "  File \"/home/linuxu/anaconda3/envs/pyspark_TTS/lib/python3.9/site-packages/speechbrain/utils/fetching.py\", line 398, in fetch\n",
      "    return link_with_strategy(fetched_file, destination, local_strategy)\n",
      "  File \"/home/linuxu/anaconda3/envs/pyspark_TTS/lib/python3.9/site-packages/speechbrain/utils/fetching.py\", line 162, in link_with_strategy\n",
      "    dst.symlink_to(src)\n",
      "  File \"/home/linuxu/anaconda3/envs/pyspark_TTS/lib/python3.9/pathlib.py\", line 1403, in symlink_to\n",
      "    self._accessor.symlink(target, self, target_is_directory)\n",
      "  File \"/home/linuxu/anaconda3/envs/pyspark_TTS/lib/python3.9/pathlib.py\", line 456, in symlink\n",
      "    return os.symlink(a, b)\n",
      "FileExistsError: [Errno 17] File exists: '/home/linuxu/.cache/huggingface/hub/models--speechbrain--tts-tacotron2-ljspeech/snapshots/d01e530d6d8e1b388c04b882305867addbed4389/hyperparams.yaml' -> '/home/linuxu/Michael/BigData/ArticleReader/checkpoints/tts-tacotron2-ljspeech/hyperparams.yaml'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:119)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:288)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "[Stage 65:>                                                         (0 + 1) / 3]\r"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_387938/4270153668.py\", line 5, in predict_batch_udf\n  File \"/home/linuxu/Michael/BigData/ArticleReader/ArticleReader/Narrator.py\", line 10, in __init__\n    self.loadModels(tts_model, vocoder_model)\n  File \"/home/linuxu/Michael/BigData/ArticleReader/ArticleReader/Narrator.py\", line 19, in loadModels\n    self.tts = Tacotron2.from_hparams(\n  File \"/home/linuxu/anaconda3/envs/pyspark_TTS/lib/python3.9/site-packages/speechbrain/inference/interfaces.py\", line 472, in from_hparams\n    hparams_local_path = fetch(\n  File \"/home/linuxu/anaconda3/envs/pyspark_TTS/lib/python3.9/site-packages/speechbrain/utils/fetching.py\", line 398, in fetch\n    return link_with_strategy(fetched_file, destination, local_strategy)\n  File \"/home/linuxu/anaconda3/envs/pyspark_TTS/lib/python3.9/site-packages/speechbrain/utils/fetching.py\", line 162, in link_with_strategy\n    dst.symlink_to(src)\n  File \"/home/linuxu/anaconda3/envs/pyspark_TTS/lib/python3.9/pathlib.py\", line 1403, in symlink_to\n    self._accessor.symlink(target, self, target_is_directory)\n  File \"/home/linuxu/anaconda3/envs/pyspark_TTS/lib/python3.9/pathlib.py\", line 456, in symlink\n    return os.symlink(a, b)\nFileExistsError: [Errno 17] File exists: '/home/linuxu/.cache/huggingface/hub/models--speechbrain--tts-tacotron2-ljspeech/snapshots/d01e530d6d8e1b388c04b882305867addbed4389/hyperparams.yaml' -> '/home/linuxu/Michael/BigData/ArticleReader/checkpoints/tts-tacotron2-ljspeech/hyperparams.yaml'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[176], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m step4\u001b[39m.\u001b[39;49mselect(F\u001b[39m.\u001b[39;49msize(\u001b[39m\"\u001b[39;49m\u001b[39mwaveform\u001b[39;49m\u001b[39m\"\u001b[39;49m))\u001b[39m.\u001b[39;49mshow()\n",
      "File \u001b[0;32m~/anaconda3/envs/pyspark_TTS/lib/python3.9/site-packages/pyspark/sql/dataframe.py:947\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mshow\u001b[39m(\u001b[39mself\u001b[39m, n: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m20\u001b[39m, truncate: Union[\u001b[39mbool\u001b[39m, \u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m, vertical: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    888\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[1;32m    889\u001b[0m \n\u001b[1;32m    890\u001b[0m \u001b[39m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[39m    name | Bob\u001b[39;00m\n\u001b[1;32m    946\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_show_string(n, truncate, vertical))\n",
      "File \u001b[0;32m~/anaconda3/envs/pyspark_TTS/lib/python3.9/site-packages/pyspark/sql/dataframe.py:965\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    959\u001b[0m     \u001b[39mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    960\u001b[0m         error_class\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNOT_BOOL\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    961\u001b[0m         message_parameters\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39marg_name\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mvertical\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39marg_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mtype\u001b[39m(vertical)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[1;32m    962\u001b[0m     )\n\u001b[1;32m    964\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(truncate, \u001b[39mbool\u001b[39m) \u001b[39mand\u001b[39;00m truncate:\n\u001b[0;32m--> 965\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mshowString(n, \u001b[39m20\u001b[39;49m, vertical)\n\u001b[1;32m    966\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    967\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pyspark_TTS/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/pyspark_TTS/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_387938/4270153668.py\", line 5, in predict_batch_udf\n  File \"/home/linuxu/Michael/BigData/ArticleReader/ArticleReader/Narrator.py\", line 10, in __init__\n    self.loadModels(tts_model, vocoder_model)\n  File \"/home/linuxu/Michael/BigData/ArticleReader/ArticleReader/Narrator.py\", line 19, in loadModels\n    self.tts = Tacotron2.from_hparams(\n  File \"/home/linuxu/anaconda3/envs/pyspark_TTS/lib/python3.9/site-packages/speechbrain/inference/interfaces.py\", line 472, in from_hparams\n    hparams_local_path = fetch(\n  File \"/home/linuxu/anaconda3/envs/pyspark_TTS/lib/python3.9/site-packages/speechbrain/utils/fetching.py\", line 398, in fetch\n    return link_with_strategy(fetched_file, destination, local_strategy)\n  File \"/home/linuxu/anaconda3/envs/pyspark_TTS/lib/python3.9/site-packages/speechbrain/utils/fetching.py\", line 162, in link_with_strategy\n    dst.symlink_to(src)\n  File \"/home/linuxu/anaconda3/envs/pyspark_TTS/lib/python3.9/pathlib.py\", line 1403, in symlink_to\n    self._accessor.symlink(target, self, target_is_directory)\n  File \"/home/linuxu/anaconda3/envs/pyspark_TTS/lib/python3.9/pathlib.py\", line 456, in symlink\n    return os.symlink(a, b)\nFileExistsError: [Errno 17] File exists: '/home/linuxu/.cache/huggingface/hub/models--speechbrain--tts-tacotron2-ljspeech/snapshots/d01e530d6d8e1b388c04b882305867addbed4389/hyperparams.yaml' -> '/home/linuxu/Michael/BigData/ArticleReader/checkpoints/tts-tacotron2-ljspeech/hyperparams.yaml'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.fetching:Fetch model.ckpt: Fetching from HuggingFace Hub 'speechbrain/tts-tacotron2-ljspeech' if not cached\n",
      "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: model\n",
      "/home/linuxu/anaconda3/envs/pyspark_TTS/lib/python3.9/site-packages/speechbrain/utils/checkpoints.py:200: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(path, map_location=device)\n",
      "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/tts-hifigan-ljspeech' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/tts-hifigan-ljspeech' if not cached\n",
      "/home/linuxu/anaconda3/envs/pyspark_TTS/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "INFO:speechbrain.utils.fetching:Fetch generator.ckpt: Fetching from HuggingFace Hub 'speechbrain/tts-hifigan-ljspeech' if not cached\n",
      "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: generator\n",
      "     running TTS model\n",
      "     batch size:  9\n",
      "INFO:speechbrain.utils.quirks:Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]\n",
      "INFO:speechbrain.utils.quirks:Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1291, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/usr/local/spark/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "     TTS model finished\n",
      "     running vocoder model\n",
      "25/03/13 20:08:16 WARN ArrowPythonRunner: Incomplete task 1.0 in stage 65 (TID 50) interrupted: Attempting to kill Python Worker\n",
      "25/03/13 20:08:16 WARN BlockManager: Putting block rdd_282_1 failed due to exception org.apache.spark.TaskKilledException.\n",
      "25/03/13 20:08:16 WARN BlockManager: Block rdd_282_1 could not be removed as it was not found on disk or in memory\n",
      "25/03/13 20:08:16 WARN TaskSetManager: Lost task 1.0 in stage 65.0 (TID 50) (192.168.90.110 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 65.0 failed 1 times, most recent failure: Lost task 0.0 in stage 65.0 (TID 49) (192.168.90.110 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_387938/4270153668.py\", line 5, in predict_batch_udf\n",
      "  File \"/home/linuxu/Michael/BigData/ArticleReader/ArticleReader/Narrator.py\", line 10, in __init__\n",
      "    self.loadModels(tts_model, vocoder_model)\n",
      "  File \"/home/linuxu/Michael/BigData/ArticleReader/ArticleReader/Narrator.py\", line 19, in loadModels\n",
      "    self.tts = Tacotron2.from_hparams(\n",
      "  File \"/home/linuxu/anaconda3/envs/pyspark_TTS/lib/python3.9/site-packages/speechbrain/inference/interfaces.py\", line 472, in from_hparams\n",
      "    hparams_local_path = fetch(\n",
      "  File \"/home/linuxu/anaconda3/envs/pyspark_TTS/lib/python3.9/site-packages/speechbrain/utils/fetching.py\", line 398, in fetch\n",
      "    return link_with_strategy(fetched_file, destination, local_strategy)\n",
      "  File \"/home/linuxu/anaconda3/envs/pyspark_TTS/lib/python3.9/site-packages/speechbrain/utils/fetching.py\", line 162, in link_with_strategy\n",
      "    dst.symlink_to(src)\n",
      "  File \"/home/linuxu/anaconda3/envs/pyspark_TTS/lib/python3.9/pathlib.py\", line 1403, in symlink_to\n",
      "    self._accessor.symlink(target, self, target_is_directory)\n",
      "  File \"/home/linuxu/anaconda3/envs/pyspark_TTS/lib/python3.9/pathlib.py\", line 456, in symlink\n",
      "    return os.symlink(a, b)\n",
      "FileExistsError: [Errno 17] File exists: '/home/linuxu/.cache/huggingface/hub/models--speechbrain--tts-tacotron2-ljspeech/snapshots/d01e530d6d8e1b388c04b882305867addbed4389/hyperparams.yaml' -> '/home/linuxu/Michael/BigData/ArticleReader/checkpoints/tts-tacotron2-ljspeech/hyperparams.yaml'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:119)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:288)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n"
     ]
    }
   ],
   "source": [
    "step4.select(F.size(\"waveform\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0ac368",
   "metadata": {},
   "outputs": [],
   "source": [
    "final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fe2b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "wfc = wf.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e5b0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wfc.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c86add9",
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform = wfc.collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca52c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "narrator = Narrator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3f9a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "tens = torch.Tensor(waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2111e6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b02696f",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_file = \"output/spark_test\"\n",
    "\n",
    "\n",
    "print(\"saving sound\")\n",
    "narrator.save_audio(case_file + \".wav\",tens )\n",
    "print(\"done saving sound\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007a660c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_batch_size(memory_per_sentence, total_memory, safety_factor=0.8):\n",
    "    # safety_factor reserves a buffer (e.g., 80% of total memory)\n",
    "    return int((total_memory * safety_factor) / memory_per_sentence)\n",
    "\n",
    "batch_size = calculate_batch_size(memory_per_sentence=500_000_000, total_memory=16_000_000_000)\n",
    "batch_size\n",
    "# Result: batch_size = 25"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6225add0",
   "metadata": {},
   "source": [
    "## Regular UDF approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777344c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_chunks = psp.from_pandas(chunker.get_test_batch(3,53))\n",
    "#chunks = spark.from_pandas(chunker.chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbac1e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(n_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9426206c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_chunks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bb08a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Estimate batch size\n",
    "batch_size = calculate_batch_size(memory_per_sentence=500_000_000, total_memory=16_000_000_000)\n",
    "batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c13003",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_chunks.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054604c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Repartition the DataFrame to match the batch size\n",
    "num_partitions = max(1, n_chunks.sentence.count() // batch_size)\n",
    "num_partitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261292df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from pyspark import SparkContext\n",
    "# import torch\n",
    "\n",
    "# sc = SparkContext.getOrCreate()\n",
    "\n",
    "# # Load your PyTorch model\n",
    "# model = torch.load('path_to_model.pth', map_location='cpu')\n",
    "# model.eval()\n",
    "\n",
    "# Broadcast the model\n",
    "#broadcast_model = sc.broadcast(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac16413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the structure and naming for newly created columns\n",
    "schema = StructType([StructField(\"waveforms\",\n",
    "                                 IntegerType(), False),\n",
    "                     StructField(\"mel_lengths\",\n",
    "                                 IntegerType(), False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ccd7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf('string', PandasUDFType.SCALAR)\n",
    "def tts_udf(batch):\n",
    "    narrator = Narrator()    \n",
    "    waveforms, mel_lengths = narrator.infer(batch)    \n",
    "    return waveforms, mel_lengths\n",
    "\n",
    "\n",
    "    # ensure sentences are sorted by seq_len\n",
    "    batch_df.loc[:,\"seq_len\"] = batch_df.sentence.map(narrator.seq_len)\n",
    "    batch_df.sort_values(\"seq_len\", ascending=False, inplace=True)\n",
    "\n",
    "    # # defining pauses between paragraphs                \n",
    "    # mel_lengths = narrator.add_pauses(batch_df.sentence, mel_lengths, pause_dur=40)        \n",
    "\n",
    "    # # turning tensor into regular array\n",
    "    # arr = torch.tensor_split(waveforms.squeeze(1), len(waveforms), dim=0)\n",
    "\n",
    "    # # cut padding\n",
    "    # arr = [a[:, :l] for a, l in zip(arr, mel_lengths * narrator.hop_len)]\n",
    "    \n",
    "    # mel_lengths = mel_lengths.detach().numpy()\n",
    "    # batch_df[\"waveform\"] = arr\n",
    "    # batch_df[\"mel_lengths\"] = mel_lengths\n",
    "    # batch_df[\"durations_sec\"] = mel_lengths / 22050.0\n",
    "    # return batch_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970fe7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the UDF\n",
    "step1 = n_chunks.to_spark().sort(\"text_len\", ascending=False ) \n",
    "step1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875630eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(step1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0b87bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "step2 = step1.withColumn(\"Result\", tts_udf(\"sentence\")) \n",
    "step2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316b6344",
   "metadata": {},
   "outputs": [],
   "source": [
    "step3 = step2.select(\"index\", \"sentence\", \"text_len\", \"Result.*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bc19cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf('string', PandasUDFType.SCALAR)\n",
    "def tts_udf(batch):\n",
    "    model = broadcast_model.value.to('cuda')\n",
    "    memory_per_sentence = 500_000_000  # Estimate\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory * 0.8\n",
    "    batch_size = int(total_memory / memory_per_sentence)\n",
    "\n",
    "    results = []\n",
    "    for i in range(0, len(batch), batch_size):\n",
    "        batch = batch[i:i + batch_size]\n",
    "        with torch.no_grad():\n",
    "            inputs = [torch.tensor(sentence).to('cuda') for sentence in batch]\n",
    "            outputs = model(inputs)\n",
    "            results.extend(outputs.cpu().numpy())\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9d3237",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to process sentences in a batch\n",
    "def process_batch(partition):\n",
    "    model = broadcast_model.value.to('cuda')\n",
    "    results = []\n",
    "    for sentence in partition:\n",
    "        with torch.no_grad():\n",
    "            output = model(sentence)\n",
    "            results.append(output.cpu().numpy())\n",
    "    return iter(results)\n",
    "\n",
    "# Apply the function to each partition\n",
    "processed_rdd = df.rdd.mapPartitions(process_batch)\n",
    "result_df = processed_rdd.toDF([\"processed_output\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f0016f49",
   "metadata": {},
   "source": [
    "Set the executor's memory and GPU resources:\n",
    "\n",
    "```\n",
    "--conf spark.executor.memory=16G\n",
    "--conf spark.executor.resource.gpu.amount=1\n",
    "```\n",
    "\n",
    "Adjust the number of partitions:\n",
    "```\n",
    "--conf spark.sql.shuffle.partitions=<num_partitions>\n",
    "```\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44d9e29d-342a-416e-963b-d20ef8983cfc",
   "metadata": {},
   "source": [
    "# End"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "550f119b",
   "metadata": {},
   "source": [
    "# Trash "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd54de80",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_df = chunker.get_test_batch(3,53)\n",
    "batch_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76584cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_df.loc[:,\"seq\"] = batch_df.sentence.map(narrator.tts.text_to_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e264f203",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ed6ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_df.seq.map(lambda s: len(s[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe68ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = batch_df.sort_values(\"text_len\" ,ascending = False).reset_index()\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6644ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "narrator = Narrator()    \n",
    "waveforms, mel_lengths = narrator.infer(sentences.sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57ae8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "waveforms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76674434",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = torch.tensor_split(waveforms.squeeze(1), len(waveforms), dim=0)\n",
    "arr = [a[0].squeeze(0).numpy() for a in arr]\n",
    "# cut padding\n",
    "#arr = [a[:, :l] for a, l in zip(arr, mel_lengths * narrator.hop_len)]\n",
    "\n",
    "#mel_lengths = mel_lengths.detach().numpy()\n",
    "# batch_df[\"waveform\"] = arr\n",
    "# batch_df[\"mel_lengths\"] = mel_lengths\n",
    "# batch_df[\"durations_sec\"] = mel_lengths / 22050.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864b667a",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e3e4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "output = pd.DataFrame({\"waveform\": arr, \"mel_lengths\": mel_lengths})\n",
    "# sentences[\"waveforms\"] = waveforms\n",
    "# sentences[\"mel_lengths\"] = mel_lengths    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079947d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2362a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217a7cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.loc[0,\"waveform\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_TTS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
